<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS180 Project 5</title>
    <style>
        .image-row {
            display: flex;
            justify-content: space-between;
        }

        .image-row img {
            width: 30%; 
            margin: 0 10px;
        }
    </style>
</head>
<body>
    <h1>Maochuan Lu - CS180 Project 5</h1>

    <h2 style="font-size: 30px;">Part A: Overview</h2>
    <p style="font-size: 20px;">
       In Project5 part A, I will play around with diffusion models, implement diffusion sampling loops, and use them for other tasks such as inpainting and creating optical illusions. 
    </p>

    <h2 style="font-size: 30px;">Downloading Precomputed Text Embeddings</h2>
    <p style="font-size: 20px;">
        I use seeds = 180, and num_inference_steps = 10, 20, and 40, for the following pictures, 
        We can see that increasing num_inference_steps generally enhances the detail and clarity of generated images, also they all represent their corresponding prompt except the detail in the generated image is different: 
        at num_inference_steps = 10, images tend to be less detailed and may appear rough, with features like the hat being less defined, we can see that the man with hat looks like in a oil painting rather than a real photo.
        With 20 steps, there's a improvement, it shows a better clarity and more detail, we can see that the features in generated images are more recognizable.
        At 40 steps, images are at their most refined, with highly detailed textures, sharper edges, and natural shading, making features like the hat look more realistic.
    </p> 
    <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <h3>num_inference_steps = 10</h3>
        <img src="proj5a_pic/inference_step=10.png" alt="Image with 10 Inference Steps" width="800">
    </div>
    <div style="margin-bottom: 20px;">
        <h3>num_inference_steps = 20</h3>
        <img src="proj5a_pic/inference_step=20.png" alt="Image with 20 Inference Steps" width="800">
    </div>
    <div style="margin-bottom: 20px;">
        <h3>num_inference_steps = 40</h3>
        <img src="proj5a_pic/inference_step=40.png" alt="Image with 40 Inference Steps" width="800">
    </div>
</div>
    
    <h2 style="font-size: 30px;">1.1 Implementing the Forward Process</h2>
    <p style="font-size: 20px;">
        I generates noisy images of test_im at different noise levels based on specified timesteps (t=250, t=500, and t=750). 
        The forward function adds Gaussian noise to the image according to each timestep's cumulative alpha value. 
    </p> 
    <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.1.png" alt="1.1" width="800">
    </div>
    </div>
    

    <h2 style="font-size: 30px;">1.2 Classical Denoising</h2>
    <p style="font-size: 20px;">
        I generates noisy images of campanile at different timesteps (t=250, t=500, t=750), then applies Gaussian blur to each.
    </p> 
    
    <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <h3>Original campanile</h3>
        <img src="proj5a_pic/campanile.jpg" alt="original" width="200">
    </div>
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.2.png" alt="1.2" width="800">
    </div>
    </div>

    <h2 style="font-size: 30px;">1.3 One-Step Denoising</h2>
    <p style="font-size: 20px;">
       In this part, I generate noisy images at t=250, t=500, t=750). For each noisy image, I estimate the noise using a U-Net model with the prompt embedding "a high quality photo" 
        and performs one-step denoising to approximate a clean image.
    </p> 
    
    <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.3.png" alt="1.3" width="800">
    </div>
    </div>

    <h2 style="font-size: 30px;">1.4 Implementing Iterative Denoising</h2>
    <p style="font-size: 20px;">
       I implement iterative denoising on a noisy version of test_im using a U-Net model with a prompt embedding. 
        I first start by adding noise to test_im and then performs iterative denoising over several timesteps.
        Specifically, in the iterative_denoise function, each iteration calculates a progressively less noisy version of the image by estimating and removing noise. 
        For each timestep t, I use the formula provided to compute alpha and beta values based on cumulative noise, 
        then uses the U-Net model to predict the noise in the current image. 
        This predicted noise is subtracted to estimate a x0, and a weighted average between this estimate and the previous image gives the pred_prev_image for the next timestep. 
    </p> 
    
    <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.4.png" alt="1.4" width="1200">
    </div>
    </div>
    

    <h2 style="font-size: 30px;">1.5 Diffusion Model Sampling</h2>
    <p style="font-size: 20px;">
      I generate five random noisy images and iteratively denoises each one using a U-Net model with the text prompt "a high quality photo". 
        For each noisy image, I use the iterative_denoise function to progressively reduce noise over a series of timesteps, resulting in cleaner images.  
    </p> 
    
    <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.5.png" alt="1.5" width="800">
    </div>
    </div>

    <h2 style="font-size: 30px;">1.6 Classifier Free Guidance</h2>
    <p style="font-size: 20px;">
      The quality of previous part's image are not very good, so I use CFG to improve. It is really similar to iteratively denosing.
    Basically, I denoise five randomly generated noisy images, refining each towards a cleaner version guided by a specific prompt. 
        It first defines conditional and unconditional embeddings for CFG. Then I progressively removes noise from each image across a series of timesteps by combining noise estimates from both conditional and unconditional models by using the formula of CFG. 
        In this case, we can direct the denoising process towards the desired prompt ("a high quality photo").
        
    </p> 
    
    <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.6.png" alt="1.6" width="800">
    </div>
    </div>

    <h2 style="font-size: 30px;">1.7 Image-to-image Translation</h2>
    <p style="font-size: 20px;">
        I use SDEdit with CFG to progressively refine a noisy image toward a clean version, guided by the prompt "a high quality photo". 
        I first iteratively add noise at different starting timesteps (1, 3, 5, 7, 10, and 20) to create increasingly noisy versions of the original image.
        For each noisy image, I use the iterative_denoise_cfg function to apply iterative denoising, conditioned on the prompt embedding, to gradually remove noise and reconstruct a cleaner image. 
    </p> 
    <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <h3>Campanile</h3>
        <img src="proj5a_pic/1.71.png" alt="1.71" width="1000">
    </div>
    <div style="margin-bottom: 20px;">
        <h3>California Hall</h3>
        <img src="proj5a_pic/1.72.png" alt="1.72" width="1000">
    </div>
    <div style="margin-bottom: 20px;">
        <h3>VLSB</h3>
        <img src="proj5a_pic/1.73.png" alt="1.73" width="1000">
    </div>
    </div>

    <h2 style="font-size: 30px;">1.7.1 Editing Hand-Drawn and Web Images</h2>
    <p style="font-size: 20px;">
        In this part, I use one image from the web and two hand-drawn images. With the provided tools, I basically use SDEdit with CFG to denoise a progressively noisier version of web or hand-drawn image
        using the prompt "a high quality photo". For each starting noise level (i_start=1, 3, 5, 7, 10, and 20), I add noise to these images, then uses iterative_denoise_cfg to gradually remove the noise and produce a clean image. 
    </p> 
    <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <h3>Original Web Image</h3>
        <img src="proj5a_pic/1.711.png" alt="1.711" width="180">
    </div>
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.712.png" alt="1.712" width="800">
    </div>
        
    <div style="margin-bottom: 20px;">
        <h3>Original Hand-Drawn Image</h3>
        <img src="proj5a_pic/1.713.png" alt="1.713" width="180">
    </div>
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.714.png" alt="1.714" width="800">
    </div>

    <div style="margin-bottom: 20px;">
        <h3>Original Hand-Drawn Image</h3>
        <img src="proj5a_pic/1.715.png" alt="1.715" width="180">
    </div>
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.716.png" alt="1.716" width="800">
    </div>
    </div>

    <h2 style="font-size: 30px;">1.7.2 Inpainting</h2>
    <p style="font-size: 20px;">
        In this part, I perform inpainting on 3 images by filling in masked regions using a guided denoising process. 
        The inpaint function starts with a noisy image generated from input image and applies CFG to denoise it over multiple timesteps, using the prompt "a high quality photo".
        For each timestep, just like before, I calculate noise estimates from both the conditional and unconditional U-Net models. 
        Then I use CFG to refine the noise estimate. Using the formula provided, In masked areas, the updated image is retained, while in non-masked areas, the original image is preserved using the mask. 
    </p> 
    <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <h3>Campanile</h3>
        <img src="proj5a_pic/1.7.2.png" alt="1.7222" width="700">
    </div>
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.7.21.png" alt="1.721" width="250">
    </div>
        
    <div style="margin-bottom: 20px;">
        <<h3>Space Neddle</h3>
        <img src="proj5a_pic/1.7.22.png" alt="1.7222" width="700">
    </div>
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.7.23.png" alt="1.723" width="250">
    </div>

    <div style="margin-bottom: 20px;">
        <h3>Lake</h3>
        <img src="proj5a_pic/1.7.24.png" alt="1.724" width="700">
    </div>
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.7.25.png" alt="1.725" width="250">
    </div>
    </div>

    <h2 style="font-size: 30px;">1.7.3 Text-Conditional Image-to-image Translation</h2>
    <p style="font-size: 20px;">
       In this part, I denoise a noisy img using the prompt "a rocket ship" and CFG. 
        For each noise level (i_start=1, 3, 5, 7, 10, and 20), I add noise to input img, then I apply iterative_denoise_cfg to refine the image based on the prompt. 
    </p> 
    <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <h3>Campanile</h3>
        <img src="proj5a_pic/campanile.jpg" alt="campanile" width="200">
    </div>
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.7.3.1.png" alt="1.731" width="800">
    </div>
        
    <div style="margin-bottom: 20px;">
        <<h3>Space Neddle</h3>
        <img src="proj5a_pic/needle.jpg" alt="needle" width="200">
    </div>
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.7.3.2.png" alt="1.732" width="800">
    </div>

    <div style="margin-bottom: 20px;">
        <h3>Hoover Tower</h3>
        <img src="proj5a_pic/stf.jpeg" alt="stf" width="200">
    </div>
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.7.3.3.png" alt="1.733" width="800">
    </div>
    </div>

    <h2 style="font-size: 30px;">1.8 Visual Anagrams</h2>
    <p style="font-size: 20px;">
       In this part, I create a mirrored "flip illusion" effect by iteratively denoising an image using two different prompt embeddings: one for the original image and one for its flipped version. 
        The function make_flip_illusion denoises the image over a series of timesteps, alternating between the original and flipped image versions: 
        For each timestep, it applies CFG just like before to denoise on both the original image and its flipped version, using separate prompt embeddings.
        Then it use the formula provided to average two noise generated from two prompt embeddings. Then following things is same as before, it gradually refines the image based on the combined effect of the prompts and their flips, 
        creating a balanced, mirrored "flip illusion" effect in the final denoised image.
    </p> 
    <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 20px; text-align: center;">
    <div style="flex: 0 0 45%; max-width: 800px;">
        <img src="proj5a_pic/1.8.1.png" alt="1.8.1" width="40%">
    </div>
    <div style="flex: 0 0 45%; max-width: 800px;">
        <img src="proj5a_pic/1.8.2.png" alt="1.8.2" width="40%">
    </div>
    <div style="flex: 0 0 45%; max-width: 800px; margin-top: 20px;">
        <img src="proj5a_pic/1.8.3.png" alt="1.8.3" width="40%">
    </div>
    <div style="flex: 0 0 45%; max-width: 800px; margin-top: 20px;">
        <img src="proj5a_pic/1.8.4.png" alt="1.8.4" width="40%">
    </div>
    <div style="flex: 0 0 45%; max-width: 800px; margin-top: 20px;">
        <img src="proj5a_pic/1.8.5.png" alt="1.8.5" width="40%">
    </div>
    <div style="flex: 0 0 45%; max-width: 800px; margin-top: 20px;">
        <img src="proj5a_pic/1.8.6.png" alt="1.8.6" width="40%">
    </div>
</div>

    <h2 style="font-size: 30px;">1.9 Hybrid Images</h2>
    <p style="font-size: 20px;">
       Basically, it is the same thing except we not dealing with the flip image but another image. Also, we just change our original formula from 1.8 to use low pass and high pass function
        to get the new noise. But to get a promising result, I might need to run several time.
    </p> 
    <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.9.1.png" alt="1.9.1" width="400">
    </div>
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.9.2.png" alt="1.9.2" width="400">
    </div>
    <div style="margin-bottom: 20px;">
        <img src="proj5a_pic/1.9.3.png" alt="1.9.3" width="400">
    </div>
    </div>

<h2 style="font-size: 30px;">Part B: Overview</h2>
    <p style="font-size: 20px;">
       In Project5 part B, I will train my own diffusion model on MNIST using different strategies. 
    </p>

    <h2 style="font-size: 30px;">Part 1: Training a Single-Step Denoising UNet</h2>

    <h2 style="font-size: 30px;">Part 1.1: 1.1 Implementing the UNet</h2>
    <p style="font-size: 20px;">
        In this part, I implement Conv, DownConv, UpConv, Flatten, Unflatten, ConvBlock, DownBlock, UpBlock in the colab, so no deliverable now.
    </p> 

    <h2 style="font-size: 30px;">Part 1.2: 1.2 Using the UNet to Train a Denoiser</h2>
    <p style="font-size: 20px;">
       I  show the effect of adding Gaussian noise with varying standard deviations to MNIST images, using 5 examples and 7 different noise levels [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]
    </p> 
     <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <img src="proj5b_pic/1.1.png" alt="1.1b" width="800">
    </div>
    </div>

    <h2 style="font-size: 30px;">Part 1.2.1 Training</h2>
    <p style="font-size: 20px;">
       I train an unconditional U-Net model on the MNIST dataset. The model was trained to reconstruct clean images from noisy inputs by adding Gaussian noise to the images.
        Here is the traning loss curve.
    </p> 
     <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <img src="proj5b_pic/1.4.png" alt="1.4b" width="800">
    </div>
    </div>

    <p style="font-size: 20px;">
      I also display sample results after the 1st and 5th epoch.
    </p> 
     <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <img src="proj5b_pic/1.2.png" alt="1.2b" width="700">
    </div>
    <div style="margin-bottom: 20px;">
        <img src="proj5b_pic/1.3.png" alt="1.3b" width="700">
    </div>
    </div>

    <h2 style="font-size: 30px;">Part 1.2.2 Out-of-Distribution Testing</h2>
    <p style="font-size: 20px;">
       I evaluate my pretrained U-Net model for denoising MNIST images by adding Gaussian noise with varying levels of ùúé to the test set. Here is the result: 
    </p> 
     <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <img src="proj5b_pic/1.5.png" alt="1.5b" width="700">
    </div>
    </div>

    <h2 style="font-size: 30px;">Part 2: Training a Diffusion Model</h2>

    <h2 style="font-size: 30px;">Part 2.1 Adding Time Conditioning to UNet</h2>
    <p style="font-size: 20px;">
        I basically added the time embedding to the previous architecture, following the instructions from the website.
    </p> 

    <h2 style="font-size: 30px;">Part 2.2 Training the UNet</h2>
    <p style="font-size: 20px;">
        I first implement ddpm_schedule, ddpm_forward in the colab. Then I started training it using the hyperparameters from the website. Here is the training curve.
    </p> 
     <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <img src="proj5b_pic/1.7.png" alt="1.7b" width="800">
    </div>
    </div>

    <h2 style="font-size: 30px;">Part 2.3 Sampling from the UNet</h2>
    <p style="font-size: 20px;">
        I then implement ddpm_sample, and here is the sample result after 5 epochs.
    </p> 
     <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <img src="proj5b_pic/1.6.png" alt="1.6b" width="700">
    </div>
    </div>

    <p style="font-size: 20px;">
        Here is the sample result after 20 epochs.
    </p> 
     <div style="text-align: center;">
    <div style="margin-bottom: 20px;">
        <img src="proj5b_pic/1.8.png" alt="1.6b" width="700">
    </div>
    </div>



    
    
</body>
</html>
    
